{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPJzrerSUq3kGhmwlBrCUE0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/25Iqbalhossain/Co2_series_injection/blob/main/LSTM_test_with_C02_injections.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZp5M0qcNVrh",
        "outputId": "18f6761d-07fe-4d99-bd80-6b593e98fd88"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-425f4da9233b>:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_2'] = train_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_mean'] = train_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_std'] = train_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_fft'] = fft_result_padded\n",
            "<ipython-input-1-425f4da9233b>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_smooth'] = savgol_filter(train_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_2'] = test_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_mean'] = test_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_std'] = test_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_fft'] = fft_result_padded_test\n",
            "<ipython-input-1-425f4da9233b>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_smooth'] = savgol_filter(test_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_1'] = train_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_2'] = train_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_mean'] = train_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_std'] = train_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_fft'] = fft_result_padded\n",
            "<ipython-input-1-425f4da9233b>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_smooth'] = savgol_filter(train_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_1'] = test_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_2'] = test_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_mean'] = test_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_std'] = test_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_fft'] = fft_result_padded_test\n",
            "<ipython-input-1-425f4da9233b>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_smooth'] = savgol_filter(test_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_1'] = train_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_2'] = train_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_mean'] = train_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_std'] = train_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_fft'] = fft_result_padded\n",
            "<ipython-input-1-425f4da9233b>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_smooth'] = savgol_filter(train_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_1'] = test_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_2'] = test_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_mean'] = test_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_std'] = test_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_fft'] = fft_result_padded_test\n",
            "<ipython-input-1-425f4da9233b>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_smooth'] = savgol_filter(test_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_1'] = train_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_2'] = train_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_mean'] = train_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_std'] = train_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_fft'] = fft_result_padded\n",
            "<ipython-input-1-425f4da9233b>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_smooth'] = savgol_filter(train_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_1'] = test_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_2'] = test_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_mean'] = test_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_std'] = test_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_fft'] = fft_result_padded_test\n",
            "<ipython-input-1-425f4da9233b>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_smooth'] = savgol_filter(test_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_1'] = train_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_2'] = train_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_mean'] = train_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_std'] = train_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_fft'] = fft_result_padded\n",
            "<ipython-input-1-425f4da9233b>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_smooth'] = savgol_filter(train_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_1'] = test_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_2'] = test_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_mean'] = test_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_std'] = test_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_fft'] = fft_result_padded_test\n",
            "<ipython-input-1-425f4da9233b>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_smooth'] = savgol_filter(test_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_1'] = train_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_2'] = train_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_mean'] = train_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_std'] = train_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_fft'] = fft_result_padded\n",
            "<ipython-input-1-425f4da9233b>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_smooth'] = savgol_filter(train_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_1'] = test_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_2'] = test_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_mean'] = test_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_std'] = test_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_fft'] = fft_result_padded_test\n",
            "<ipython-input-1-425f4da9233b>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_smooth'] = savgol_filter(test_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_1'] = train_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_2'] = train_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_mean'] = train_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_std'] = train_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_fft'] = fft_result_padded\n",
            "<ipython-input-1-425f4da9233b>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_smooth'] = savgol_filter(train_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_1'] = test_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_2'] = test_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_mean'] = test_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_std'] = test_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_fft'] = fft_result_padded_test\n",
            "<ipython-input-1-425f4da9233b>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_smooth'] = savgol_filter(test_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_1'] = train_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_2'] = train_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_mean'] = train_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_std'] = train_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_fft'] = fft_result_padded\n",
            "<ipython-input-1-425f4da9233b>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_smooth'] = savgol_filter(train_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_1'] = test_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_2'] = test_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_mean'] = test_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_std'] = test_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_fft'] = fft_result_padded_test\n",
            "<ipython-input-1-425f4da9233b>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_smooth'] = savgol_filter(test_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_1'] = train_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_2'] = train_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_mean'] = train_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_std'] = train_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_fft'] = fft_result_padded\n",
            "<ipython-input-1-425f4da9233b>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_smooth'] = savgol_filter(train_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_1'] = test_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_2'] = test_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_mean'] = test_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_std'] = test_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_fft'] = fft_result_padded_test\n",
            "<ipython-input-1-425f4da9233b>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_smooth'] = savgol_filter(test_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_1'] = train_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_2'] = train_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_mean'] = train_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_std'] = train_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_fft'] = fft_result_padded\n",
            "<ipython-input-1-425f4da9233b>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_smooth'] = savgol_filter(train_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_1'] = test_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_2'] = test_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_mean'] = test_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_std'] = test_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_fft'] = fft_result_padded_test\n",
            "<ipython-input-1-425f4da9233b>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_smooth'] = savgol_filter(test_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_1'] = train_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_2'] = train_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_mean'] = train_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_std'] = train_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_fft'] = fft_result_padded\n",
            "<ipython-input-1-425f4da9233b>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_smooth'] = savgol_filter(train_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_1'] = test_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_2'] = test_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_mean'] = test_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_std'] = test_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_fft'] = fft_result_padded_test\n",
            "<ipython-input-1-425f4da9233b>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_smooth'] = savgol_filter(test_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_1'] = train_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_2'] = train_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_mean'] = train_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_std'] = train_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_fft'] = fft_result_padded\n",
            "<ipython-input-1-425f4da9233b>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_smooth'] = savgol_filter(train_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_1'] = test_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_2'] = test_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_mean'] = test_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_std'] = test_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_fft'] = fft_result_padded_test\n",
            "<ipython-input-1-425f4da9233b>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_smooth'] = savgol_filter(test_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_1'] = train_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_2'] = train_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_mean'] = train_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_std'] = train_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_fft'] = fft_result_padded\n",
            "<ipython-input-1-425f4da9233b>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_smooth'] = savgol_filter(train_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_1'] = test_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_2'] = test_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_mean'] = test_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_std'] = test_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_fft'] = fft_result_padded_test\n",
            "<ipython-input-1-425f4da9233b>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_smooth'] = savgol_filter(test_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_1'] = train_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_2'] = train_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_mean'] = train_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_std'] = train_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_fft'] = fft_result_padded\n",
            "<ipython-input-1-425f4da9233b>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_smooth'] = savgol_filter(train_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_1'] = test_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_2'] = test_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_mean'] = test_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_std'] = test_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_fft'] = fft_result_padded_test\n",
            "<ipython-input-1-425f4da9233b>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_smooth'] = savgol_filter(test_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_1'] = train_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_2'] = train_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_mean'] = train_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_std'] = train_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_fft'] = fft_result_padded\n",
            "<ipython-input-1-425f4da9233b>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_smooth'] = savgol_filter(train_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_1'] = test_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_2'] = test_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_mean'] = test_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_std'] = test_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_fft'] = fft_result_padded_test\n",
            "<ipython-input-1-425f4da9233b>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_smooth'] = savgol_filter(test_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_1'] = train_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_lag_2'] = train_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_mean'] = train_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_rolling_std'] = train_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_fft'] = fft_result_padded\n",
            "<ipython-input-1-425f4da9233b>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[f'{col}_smooth'] = savgol_filter(train_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "<ipython-input-1-425f4da9233b>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_1'] = test_df[col].shift(1)\n",
            "<ipython-input-1-425f4da9233b>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_lag_2'] = test_df[col].shift(2)\n",
            "<ipython-input-1-425f4da9233b>:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_mean'] = test_df[col].rolling(window=5).mean()\n",
            "<ipython-input-1-425f4da9233b>:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_rolling_std'] = test_df[col].rolling(window=5).std()\n",
            "<ipython-input-1-425f4da9233b>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_fft'] = fft_result_padded_test\n",
            "<ipython-input-1-425f4da9233b>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df[f'{col}_smooth'] = savgol_filter(test_df[col].fillna(0), window_length=5, polyorder=2)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - loss: 18.5254 - mae: 1.3661 - val_loss: 17.2542 - val_mae: 1.4711\n",
            "Epoch 2/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 17.1325 - mae: 1.3550 - val_loss: 17.0273 - val_mae: 1.5011\n",
            "Epoch 3/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 22.1185 - mae: 1.6394 - val_loss: 16.8389 - val_mae: 1.4969\n",
            "Epoch 4/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 15.6100 - mae: 1.3248 - val_loss: 16.4902 - val_mae: 1.6564\n",
            "Epoch 5/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 17.5974 - mae: 1.5377 - val_loss: 16.2823 - val_mae: 1.6696\n",
            "Epoch 6/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 13.5266 - mae: 1.3940 - val_loss: 15.7753 - val_mae: 1.6368\n",
            "Epoch 7/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 12.7002 - mae: 1.4482 - val_loss: 15.4384 - val_mae: 1.6124\n",
            "Epoch 8/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 16.4301 - mae: 1.6343 - val_loss: 15.9615 - val_mae: 1.8524\n",
            "Epoch 9/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 16.2794 - mae: 1.6719 - val_loss: 14.6636 - val_mae: 1.5956\n",
            "Epoch 10/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 18.6696 - mae: 1.8379 - val_loss: 14.8315 - val_mae: 1.6198\n",
            "Epoch 11/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 13.3703 - mae: 1.5902 - val_loss: 14.1901 - val_mae: 1.5281\n",
            "Epoch 12/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 14.5210 - mae: 1.6038 - val_loss: 14.0327 - val_mae: 1.5680\n",
            "Epoch 13/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 10.4635 - mae: 1.4919 - val_loss: 14.5067 - val_mae: 1.5947\n",
            "Epoch 14/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 14.6998 - mae: 1.6648 - val_loss: 14.1520 - val_mae: 1.6073\n",
            "Epoch 15/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 9.6541 - mae: 1.3910 - val_loss: 15.1919 - val_mae: 1.6658\n",
            "Epoch 16/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 8.8327 - mae: 1.3389 - val_loss: 15.6253 - val_mae: 1.6016\n",
            "Epoch 17/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 11.9125 - mae: 1.5566 - val_loss: 15.0738 - val_mae: 1.7150\n",
            "Epoch 18/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 12.0282 - mae: 1.5843 - val_loss: 16.1824 - val_mae: 1.7146\n",
            "Epoch 19/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.4898 - mae: 1.1991 - val_loss: 16.5137 - val_mae: 1.7341\n",
            "Epoch 20/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 10.0870 - mae: 1.4802 - val_loss: 14.4098 - val_mae: 1.5455\n",
            "Epoch 21/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 6.8775 - mae: 1.2054 - val_loss: 15.9047 - val_mae: 1.6993\n",
            "Epoch 22/100\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 7.8042 - mae: 1.2632 - val_loss: 16.7792 - val_mae: 1.7373\n",
            "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   2.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   3.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.7; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.7; total time=   2.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.7; total time=   4.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   2.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   2.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   2.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   2.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   4.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   2.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.7; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.7; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.7; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.8; total time=   3.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.8; total time=   3.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.8; total time=   2.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=1.0; total time=   2.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=1.0; total time=   2.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=1.0; total time=   4.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.7; total time=   4.3s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.7; total time=   4.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.7; total time=   6.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.8; total time=   4.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.8; total time=   6.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.8; total time=   4.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=1.0; total time=   4.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=1.0; total time=   6.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=1.0; total time=   5.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=0.7; total time=   8.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=0.7; total time=   8.3s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=0.7; total time=   6.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=0.8; total time=   8.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=0.8; total time=   6.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=0.8; total time=   8.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=1.0; total time=   9.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=1.0; total time=   7.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=1.0; total time=   9.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   3.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   6.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   3.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   5.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   5.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   4.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=1.0; total time=   6.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=1.0; total time=   4.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=1.0; total time=   4.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=   9.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=  10.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=   9.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=   8.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=  10.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=  10.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=1.0; total time=  11.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=1.0; total time=  11.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=1.0; total time=   9.3s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.7; total time=  14.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.7; total time=  17.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.7; total time=  13.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.8; total time=  14.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.8; total time=  15.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.8; total time=  14.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=1.0; total time=  16.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=1.0; total time=  19.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=1.0; total time=  16.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.7; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=1.0; total time=   2.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=1.0; total time=   3.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.7; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.7; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.7; total time=   2.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.8; total time=   4.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.8; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.8; total time=   2.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=1.0; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=1.0; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=1.0; total time=   4.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.7; total time=   2.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.7; total time=   2.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.7; total time=   2.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.8; total time=   2.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.8; total time=   4.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.8; total time=   2.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=1.0; total time=   2.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=1.0; total time=   2.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=1.0; total time=   4.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.7; total time=   5.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.7; total time=   7.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.7; total time=   5.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.8; total time=   7.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.8; total time=   5.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.8; total time=   7.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=1.0; total time=   5.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=1.0; total time=   7.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=1.0; total time=   5.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=0.7; total time=  10.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=0.7; total time=  11.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=0.7; total time=   8.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=0.8; total time=  10.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=0.8; total time=  11.3s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=0.8; total time=  10.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=1.0; total time=  10.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=1.0; total time=   8.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=1.0; total time=  10.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=0.7; total time=   6.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=0.7; total time=   4.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=0.7; total time=   4.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=0.8; total time=   7.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=0.8; total time=   5.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=0.8; total time=   6.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=1.0; total time=   5.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=1.0; total time=   8.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=1.0; total time=   5.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=0.7; total time=  13.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=0.7; total time=  13.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=0.7; total time=  12.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=0.8; total time=  14.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=0.8; total time=  16.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=0.8; total time=  13.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=1.0; total time=  15.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=1.0; total time=  15.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=1.0; total time=  15.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=0.7; total time=  22.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=0.7; total time=  24.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=0.7; total time=  18.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=0.8; total time=  25.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=0.8; total time=  25.3s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=0.8; total time=  23.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=1.0; total time=  24.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=1.0; total time=  23.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=1.0; total time=  25.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.7; total time=   2.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.7; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   3.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.7; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.7; total time=   4.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.7; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   2.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   4.3s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.7; total time=   2.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.7; total time=   4.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.7; total time=   3.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.8; total time=   3.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.8; total time=   3.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.8; total time=   4.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=1.0; total time=   3.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=1.0; total time=   3.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=1.0; total time=   2.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.7; total time=   8.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.7; total time=   6.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.7; total time=   8.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.8; total time=   8.3s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.8; total time=   6.3s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.8; total time=   8.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=1.0; total time=   5.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=1.0; total time=   8.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=1.0; total time=   6.3s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=0.7; total time=  11.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=0.7; total time=  12.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=0.7; total time=  11.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=0.8; total time=  12.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=0.8; total time=  11.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=0.8; total time=  11.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=1.0; total time=   8.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=1.0; total time=  11.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=1.0; total time=  11.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.7; total time=   7.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.7; total time=   5.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.7; total time=   7.3s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.8; total time=   6.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.8; total time=   8.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.8; total time=   5.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=1.0; total time=   9.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=1.0; total time=   9.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=1.0; total time=   6.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=0.7; total time=  15.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=0.7; total time=  17.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=0.7; total time=  14.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=0.8; total time=  16.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=0.8; total time=  16.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=0.8; total time=  17.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=1.0; total time=  17.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=1.0; total time=  19.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=1.0; total time=  18.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=0.7; total time=  25.3s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=0.7; total time=  26.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=0.7; total time=  24.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=0.8; total time=  25.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=0.8; total time=  23.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=0.8; total time=  24.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=1.0; total time=  23.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=1.0; total time=  22.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=1.0; total time=  20.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   2.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   3.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.7; total time=   4.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.7; total time=   2.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.7; total time=   2.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   2.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   2.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   4.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   2.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   2.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   2.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.7; total time=   2.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.7; total time=   4.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.7; total time=   2.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.8; total time=   2.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.8; total time=   2.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.8; total time=   4.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=1.0; total time=   2.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=1.0; total time=   2.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=1.0; total time=   2.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.7; total time=   6.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.7; total time=   4.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.7; total time=   6.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.8; total time=   5.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.8; total time=   7.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.8; total time=   5.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=1.0; total time=   7.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=1.0; total time=   5.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=1.0; total time=   7.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=0.7; total time=   7.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=0.7; total time=   9.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=0.7; total time=   8.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=0.8; total time=   8.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=0.8; total time=   9.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=0.8; total time=   9.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=1.0; total time=   8.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=1.0; total time=  10.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=1.0; total time=  11.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   4.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   6.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   4.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   6.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   4.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   4.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=1.0; total time=   6.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=1.0; total time=   5.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=1.0; total time=   7.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=  10.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=   9.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=  10.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=  11.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=  11.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=  10.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=1.0; total time=  11.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=1.0; total time=  10.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=1.0; total time=  11.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.7; total time=  15.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.7; total time=  18.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.7; total time=  14.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.8; total time=  16.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.8; total time=  19.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.8; total time=  16.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=1.0; total time=  17.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=1.0; total time=  20.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=1.0; total time=  17.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=1.0; total time=   3.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   3.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.7; total time=   2.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.7; total time=   2.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.7; total time=   4.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.8; total time=   2.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.8; total time=   2.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.8; total time=   2.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=1.0; total time=   4.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=1.0; total time=   2.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=1.0; total time=   2.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.7; total time=   2.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.7; total time=   3.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.7; total time=   4.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.8; total time=   3.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.8; total time=   3.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.8; total time=   3.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=1.0; total time=   4.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=1.0; total time=   3.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=1.0; total time=   3.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.7; total time=   8.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.7; total time=   6.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.7; total time=   7.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.8; total time=   7.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.8; total time=   6.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.8; total time=   8.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=1.0; total time=   6.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=1.0; total time=   9.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=1.0; total time=   7.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=0.7; total time=  10.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=0.7; total time=  12.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=0.7; total time=  11.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=0.8; total time=  12.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=0.8; total time=  12.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=0.8; total time=  12.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=1.0; total time=  11.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=1.0; total time=  12.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=1.0; total time=  12.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=0.7; total time=   5.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=0.7; total time=   7.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=0.7; total time=   4.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=0.8; total time=   7.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=0.8; total time=   6.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=0.8; total time=   6.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=1.0; total time=   6.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=1.0; total time=   8.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=1.0; total time=   6.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=0.7; total time=  14.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=0.7; total time=  16.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=0.7; total time=  12.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=0.8; total time=  16.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=0.8; total time=  15.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=0.8; total time=  14.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=1.0; total time=  17.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=1.0; total time=  19.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=1.0; total time=  16.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=0.7; total time=  24.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=0.7; total time=  25.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=0.7; total time=  22.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=0.8; total time=  27.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=0.8; total time=  27.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=0.8; total time=  24.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=1.0; total time=  27.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=1.0; total time=  29.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=1.0; total time=  27.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   2.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   3.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.7; total time=   2.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.7; total time=   4.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.7; total time=   2.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   2.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   2.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   4.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   2.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   2.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   2.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.7; total time=   4.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.7; total time=   3.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.7; total time=   2.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.8; total time=   3.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.8; total time=   5.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.8; total time=   3.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=1.0; total time=   3.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=1.0; total time=   4.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=1.0; total time=   3.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.7; total time=   7.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.7; total time=   8.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.7; total time=   8.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.8; total time=   7.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.8; total time=   9.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.8; total time=   9.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=1.0; total time=   6.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=1.0; total time=   8.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=1.0; total time=   7.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=0.7; total time=  13.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=0.7; total time=  12.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=0.7; total time=  12.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=0.8; total time=  13.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=0.8; total time=  13.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=0.8; total time=  13.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=1.0; total time=  11.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=1.0; total time=  12.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=1.0; total time=  12.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.7; total time=   8.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.7; total time=   6.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.7; total time=   7.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.8; total time=   9.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.8; total time=   8.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.8; total time=   8.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=1.0; total time=   7.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=1.0; total time=  10.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=1.0; total time=   9.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=0.7; total time=  16.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=0.7; total time=  17.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=0.7; total time=  17.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=0.8; total time=  18.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=0.8; total time=  21.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=0.8; total time=  16.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=1.0; total time=  20.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=1.0; total time=  19.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=1.0; total time=  20.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=0.7; total time=  27.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=0.7; total time=  28.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=0.7; total time=  25.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=0.8; total time=  27.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=0.8; total time=  28.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=0.8; total time=  26.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=1.0; total time=  24.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=1.0; total time=  25.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=1.0; total time=  23.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   1.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   3.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   2.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   4.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.7; total time=   2.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.7; total time=   2.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.7; total time=   4.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   2.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   2.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   2.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   4.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   2.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   2.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.7; total time=   2.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.7; total time=   4.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.7; total time=   2.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.8; total time=   2.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.8; total time=   2.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.8; total time=   5.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=1.0; total time=   3.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=1.0; total time=   3.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=1.0; total time=   3.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.7; total time=   7.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.7; total time=   6.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.7; total time=   6.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.8; total time=   7.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.8; total time=   6.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.8; total time=   8.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=1.0; total time=   6.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=1.0; total time=   8.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=1.0; total time=   6.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=0.7; total time=  10.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=0.7; total time=  11.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=0.7; total time=  10.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=0.8; total time=   9.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=0.8; total time=  10.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=0.8; total time=  11.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=1.0; total time=  12.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=1.0; total time=  12.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=300, subsample=1.0; total time=  11.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   4.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   7.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   5.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   7.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   5.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   6.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=1.0; total time=   5.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=1.0; total time=   8.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=1.0; total time=   5.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=  11.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=  12.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=  11.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=  12.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=  13.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=  11.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=1.0; total time=  11.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=1.0; total time=  15.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=1.0; total time=  12.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.7; total time=  17.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.7; total time=  18.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.7; total time=  18.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.8; total time=  18.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.8; total time=  21.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.8; total time=  17.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=1.0; total time=  21.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=1.0; total time=  24.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=1.0; total time=  19.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.7; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.7; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.8; total time=   3.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   4.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.7; total time=   4.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.7; total time=   2.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.7; total time=   2.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.8; total time=   2.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.8; total time=   4.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=0.8; total time=   2.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=1.0; total time=   2.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=1.0; total time=   2.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=300, subsample=1.0; total time=   4.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.7; total time=   3.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.7; total time=   3.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.7; total time=   5.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.8; total time=   3.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.8; total time=   3.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.8; total time=   3.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=1.0; total time=   5.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=1.0; total time=   3.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=1.0; total time=   4.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.7; total time=   8.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.7; total time=   9.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.7; total time=   6.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.8; total time=   9.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.8; total time=   9.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.8; total time=   7.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=1.0; total time=  10.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=1.0; total time=  10.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=1.0; total time=   7.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=0.7; total time=  14.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=0.7; total time=  14.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=0.7; total time=  13.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=0.8; total time=  13.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=0.8; total time=  14.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=0.8; total time=  13.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=1.0; total time=  13.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=1.0; total time=  14.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=300, subsample=1.0; total time=  13.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=0.7; total time=   7.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=0.7; total time=   6.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=0.7; total time=   7.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=0.8; total time=   6.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=0.8; total time=   8.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=0.8; total time=   6.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=1.0; total time=   9.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=1.0; total time=  10.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=100, subsample=1.0; total time=   7.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=0.7; total time=  18.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=0.7; total time=  17.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=0.7; total time=  14.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=0.8; total time=  18.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=0.8; total time=  19.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=0.8; total time=  16.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=1.0; total time=  21.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=1.0; total time=  23.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=200, subsample=1.0; total time=  19.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=0.7; total time=  27.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=0.7; total time=  31.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=0.7; total time=  23.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=0.8; total time=  32.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=0.8; total time=  31.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=0.8; total time=  27.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=1.0; total time=  32.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=1.0; total time=  33.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=9, n_estimators=300, subsample=1.0; total time=  33.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.7; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.7; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.7; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   2.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   2.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   4.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   2.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.7; total time=   3.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.7; total time=   4.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.7; total time=   2.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   3.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   4.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   3.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   2.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   2.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   4.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.7; total time=   3.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.7; total time=   3.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.7; total time=   3.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.8; total time=   5.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.8; total time=   3.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.8; total time=   4.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=1.0; total time=   5.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=1.0; total time=   4.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=1.0; total time=   5.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.7; total time=   8.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.7; total time=  10.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.7; total time=  10.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.8; total time=   8.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.8; total time=  10.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.8; total time=  10.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=1.0; total time=   9.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=1.0; total time=   8.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=1.0; total time=  10.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=0.7; total time=  15.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=0.7; total time=  15.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=0.7; total time=  17.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=0.8; total time=  15.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=0.8; total time=  15.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=0.8; total time=  15.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=1.0; total time=  13.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=1.0; total time=  14.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=300, subsample=1.0; total time=  14.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.7; total time=   9.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.7; total time=   9.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.7; total time=   6.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.8; total time=   9.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.8; total time=   9.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.8; total time=   6.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=1.0; total time=  10.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=1.0; total time=  12.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=100, subsample=1.0; total time=  10.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=0.7; total time=  18.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=0.7; total time=  22.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=0.7; total time=  16.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=0.8; total time=  22.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=0.8; total time=  23.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=0.8; total time=  19.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=1.0; total time=  20.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=1.0; total time=  23.2s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=200, subsample=1.0; total time=  23.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=0.7; total time=  29.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=0.7; total time=  34.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=0.7; total time=  27.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=0.8; total time=  30.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=0.8; total time=  32.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=0.8; total time=  29.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=1.0; total time=  27.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=1.0; total time=  27.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=9, n_estimators=300, subsample=1.0; total time=  27.5s\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Validation R Score: 0.2468 (24.68%)\n",
            "Mean Absolute Error (MAE): 1.4367\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Predictions saved successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-425f4da9233b>:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_df['inj_diff_pred'] = 0.6 * lstm_test_preds + 0.4 * xgb_test_preds\n"
          ]
        }
      ],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV  # <-- Added GridSearchCV import\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "import xgboost as xgb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from scipy.fftpack import fft\n",
        "from scipy.signal import savgol_filter\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset\n",
        "train_df = pd.read_excel('/content/drive/MyDrive/Co2_Injection_Ml_Data_Xl/CO2_Injection_rate train.xlsx')\n",
        "test_df = pd.read_excel('/content/drive/MyDrive/Co2_Injection_Ml_Data_Xl/CO2_Injection_rate test exam.xlsx')\n",
        "\n",
        "# Data Cleaning and Preprocessing\n",
        "train_df.columns = train_df.columns.str.strip()  # Strip whitespace from column names\n",
        "test_df.columns = test_df.columns.str.strip()  # Strip whitespace from column names\n",
        "\n",
        "# Feature Engineering: Signal Processing + Lag Features\n",
        "numerical_features = [col for col in train_df.columns if col not in ['Date Time', 'inj_diff']]\n",
        "\n",
        "for col in numerical_features:\n",
        "    if col in train_df.columns:\n",
        "        # Adding lag features\n",
        "        train_df[f'{col}_lag_1'] = train_df[col].shift(1)\n",
        "        train_df[f'{col}_lag_2'] = train_df[col].shift(2)\n",
        "        train_df[f'{col}_rolling_mean'] = train_df[col].rolling(window=5).mean()\n",
        "        train_df[f'{col}_rolling_std'] = train_df[col].rolling(window=5).std()\n",
        "\n",
        "        # Handle FFT: Check if column is numeric and has enough data\n",
        "        numeric_col = pd.to_numeric(train_df[col], errors='coerce').dropna()\n",
        "        if len(numeric_col) > 0:\n",
        "            # Convert the Pandas Series to a NumPy array before applying FFT\n",
        "            fft_result = np.abs(fft(numeric_col.to_numpy()))\n",
        "\n",
        "            # Ensure the FFT result matches the original data length\n",
        "            # Truncate or zero-pad the FFT result to match the original length\n",
        "            fft_result_padded = np.pad(fft_result, (0, len(train_df[col]) - len(fft_result)), mode='constant')\n",
        "            train_df[f'{col}_fft'] = fft_result_padded\n",
        "        else:\n",
        "            print(f'Skipping FFT for column {col} due to insufficient data.')\n",
        "\n",
        "        # Apply Savitzky-Golay filter\n",
        "        train_df[f'{col}_smooth'] = savgol_filter(train_df[col].fillna(0), window_length=5, polyorder=2)\n",
        "\n",
        "    if col in test_df.columns:\n",
        "        # Adding lag features\n",
        "        test_df[f'{col}_lag_1'] = test_df[col].shift(1)\n",
        "        test_df[f'{col}_lag_2'] = test_df[col].shift(2)\n",
        "        test_df[f'{col}_rolling_mean'] = test_df[col].rolling(window=5).mean()\n",
        "        test_df[f'{col}_rolling_std'] = test_df[col].rolling(window=5).std()\n",
        "\n",
        "        # Handle FFT: Check if column is numeric and has enough data\n",
        "        numeric_col_test = pd.to_numeric(test_df[col], errors='coerce').dropna()\n",
        "        if len(numeric_col_test) > 0:\n",
        "            # Convert the Pandas Series to a NumPy array before applying FFT\n",
        "            fft_result_test = np.abs(fft(numeric_col_test.to_numpy()))\n",
        "\n",
        "            # Ensure the FFT result matches the original data length\n",
        "            fft_result_padded_test = np.pad(fft_result_test, (0, len(test_df[col]) - len(fft_result_test)), mode='constant')\n",
        "            test_df[f'{col}_fft'] = fft_result_padded_test\n",
        "        else:\n",
        "            print(f'Skipping FFT for column {col} due to insufficient data.')\n",
        "\n",
        "        # Apply Savitzky-Golay filter\n",
        "        test_df[f'{col}_smooth'] = savgol_filter(test_df[col].fillna(0), window_length=5, polyorder=2)\n",
        "\n",
        "# Dropping NaN values caused by shifting\n",
        "train_df.dropna(inplace=True)\n",
        "\n",
        "# Splitting the data\n",
        "X = train_df.drop(columns=['inj_diff', 'Date Time'])\n",
        "y = train_df['inj_diff']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Reshape for LSTM (samples, time steps, features)\n",
        "X_train_lstm = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
        "X_test_lstm = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n",
        "\n",
        "# Build LSTM Model\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, return_sequences=True, input_shape=(1, X_train.shape[1])),\n",
        "    Dropout(0.3),\n",
        "    LSTM(32),\n",
        "    Dropout(0.2),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train LSTM\n",
        "lstm_model.fit(X_train_lstm, y_train, epochs=100, batch_size=16, validation_data=(X_test_lstm, y_test),\n",
        "               verbose=1, callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)])\n",
        "\n",
        "# XGBoost Hyperparameter tuning with GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'subsample': [0.7, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.7, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=xgb.XGBRegressor(random_state=42), param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best XGBoost Model\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "\n",
        "# Making Predictions\n",
        "lstm_preds = lstm_model.predict(X_test_lstm).flatten()\n",
        "xgb_preds = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Hybrid Prediction (Weighted Average)\n",
        "final_preds = 0.6 * lstm_preds + 0.4 * xgb_preds\n",
        "\n",
        "# Calculate R Score and Accuracy Percentage\n",
        "r2 = r2_score(y_test, final_preds)\n",
        "mae = mean_absolute_error(y_test, final_preds)\n",
        "accuracy_percentage = r2 * 100\n",
        "print(f'Validation R Score: {r2:.4f} ({accuracy_percentage:.2f}%)')\n",
        "print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
        "\n",
        "# Predicting on Test Data\n",
        "test_features = test_df.drop(columns=['Date Time', 'inj_diff'], errors='ignore')\n",
        "test_features = scaler.transform(test_features[X.columns])\n",
        "test_features_lstm = test_features.reshape(test_features.shape[0], 1, test_features.shape[1])\n",
        "\n",
        "lstm_test_preds = lstm_model.predict(test_features_lstm).flatten()\n",
        "xgb_test_preds = best_xgb_model.predict(test_features)\n",
        "test_df['inj_diff_pred'] = 0.6 * lstm_test_preds + 0.4 * xgb_test_preds\n",
        "\n",
        "# Saving Results\n",
        "test_df[['Date Time', 'inj_diff_pred']].to_csv('CO2_injection_predictions.csv', index=False)\n",
        "print(\"Predictions saved successfully.\")\n"
      ]
    }
  ]
}